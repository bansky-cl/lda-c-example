{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\cl\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.452 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# 读取停用词列表\n",
    "def get_stopword_list(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:    # \n",
    "        stopword_list = [word.strip('\\n') for word in f.readlines()]\n",
    "    return stopword_list\n",
    "\n",
    "\n",
    "def is_chinese(uchar):\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def format_str(content):\n",
    "    content_str = ''\n",
    "    for i in content:\n",
    "        if is_chinese(i):\n",
    "            content_str = content_str + i\n",
    "    return content_str\n",
    "\n",
    "vocab = {}\n",
    "vocab_id = 0\n",
    "\n",
    "# 分词 然后清除停用词语\n",
    "def clean_stopword(str, stopword_list):\n",
    "    \n",
    "    # 预处理\n",
    "    str = str.lower().replace('digit','').replace('http','').replace('\\n','')\n",
    "    str = re.sub(r\"\\d+\", '',str) # delete digit\n",
    "    # 分词\n",
    "    clean_word_list = []\n",
    "    # word_list = jieba.lcut(str,c、ut_all=True)   # 分词后返回一个列表  jieba.cut(）   返回的是一个迭代器\n",
    "    word_list = jieba.lcut(str) # 精确模式\n",
    "    # s = ' '.join(str.split())\n",
    "    # word_list = s.split(' ') # 空格分\n",
    "\n",
    "\n",
    "    # 去停用词\n",
    "    for w in word_list:\n",
    "        if w not in stopword_list:\n",
    "            if len(w) != 1 :# 过滤单字体 \n",
    "                clean_word_list.append(w)\n",
    "\n",
    "    return clean_word_list # 返回一个token_list\n",
    "\n",
    "\n",
    "stopword_file =r'stopword.txt'\n",
    "stopword_list = get_stopword_list(stopword_file)\n",
    "\n",
    "data_file = r'weibo_data_5k.txt'\n",
    "vocab_file = r'vocab.txt'\n",
    "output_file = r'output.txt'\n",
    "freq_file = r'freq.txt'\n",
    "\n",
    "# 构建词典\n",
    "with open(data_file, encoding='utf-8',mode='r') as ipf,  open(vocab_file, encoding='utf-8',mode='w') as vbf:\n",
    "    all_word= []\n",
    "    for line in ipf:\n",
    "        # 小写 去数字 去空\n",
    "        word_list = clean_stopword(line, stopword_list) # 构造vocab 返回当前 一个文档的词频表\n",
    "        all_word += word_list\n",
    "    ipf.close()\n",
    "\n",
    "# nltk 构建词表\n",
    "cfd = nltk.FreqDist(all_word)\n",
    "\n",
    "# 保存词频\n",
    "with open(freq_file, encoding='utf-8',mode='w') as frqf:\n",
    "    for k,v in sorted(cfd.items(),key=lambda x:x[1], reverse=True)[:2000]:\n",
    "        frqf.write(k + ' ' + str(v))\n",
    "        frqf.write('\\n')\n",
    "    frqf.close()\n",
    "\n",
    "# 选取词频前2k\n",
    "vocab2k = {}\n",
    "count = 0\n",
    "for e in cfd:\n",
    "    if count < 50:\n",
    "        vocab2k[e] = count\n",
    "        count += 1\n",
    "   \n",
    "# 保存词表\n",
    "filename = open(vocab_file,'w') #dict转txt\n",
    "for k,v in vocab2k.items():\n",
    "    filename.write(k)\n",
    "    filename.write('\\n')\n",
    "filename.close()\n",
    "\n",
    "# 读取词表\n",
    "with open(data_file, encoding='utf-8',mode='r') as ipf,  open(output_file, encoding='utf-8',mode='w') as opf:\n",
    "    count = 0\n",
    "    for line in ipf:\n",
    "        word_list = clean_stopword(line, stopword_list) \n",
    "        temp_dict = {}\n",
    "        for w in word_list:\n",
    "            if w in vocab2k: # 前2k的\n",
    "                w_id  = vocab2k.get(w)\n",
    "                # print(w_id)\n",
    "                if w not in temp_dict:\n",
    "                    temp_dict[w_id] = 1\n",
    "                else:\n",
    "                    temp_dict[w_id] += 1         \n",
    "        dict_str = str(temp_dict)   \n",
    "        dict_str = str(temp_dict).replace(r\"{\",'').replace(r\"}\",'').replace(r\": \",':').replace(r\", \",' ')\n",
    "        count = dict_str.count(\":\")\n",
    "        opf.write(str(count) + ' ' + dict_str + '\\n')\n",
    "    ipf.close()\n",
    "    opf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PCenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
